{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training with Sagemaker Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is derived from the original, larger notebook, [Sagemaker bring your own model with script mode](\n",
    "https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-script-mode)\n",
    "\n",
    "In the previous notebook [pytorch-nn-multiclass-classifier](https://github.com/sjaffry/multiclass-classifier-pytorch-nn/blob/main/pytorch-nn-multiclass-classifier.ipynb), I built a simple neural net with PyTorch and trained it within the notebook that was running on a GPU instance. What I wanted to do next was to use Sagemaker managed infrasturcture for training my model so I can scale the process better and lay the foundations for consistent production deployment processes. This notebook will demonstrate how you can do that by bring your model into Sagemaker using custom training and inference scripts with SageMaker's prebuilt container for PyTorch.\n",
    "\n",
    "This notebook will show my step by step journey of how I took something that was running completely on a notebook [pytorch-nn-multiclass-classifier](https://github.com/sjaffry/multiclass-classifier-pytorch-nn/blob/main/pytorch-nn-multiclass-classifier.ipynb) and refactored it to split training into Sagemaker's managed training infrastructure.\n",
    "\n",
    "This example does not cover model deployment & inference. I will cover that in a subsequent example notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites  \n",
    "\n",
    "1. An S3 bucket for storing your model training code & data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding the architecture of Sagemaker training infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I started refactoring my code using the example notebooks in Sagemaker examoles, I wan to understand how the \"magic\" works behind the Sagemaker PyTorch SDK. So this is what I discovered:  \n",
    "\n",
    "![Sagemaker PyTorch SDK](images/img_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Review required and redundant libraries & imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do I need to download and install into this notebook any of the libraries that I did originally?  \n",
    "  \n",
    "  Contents of my original \"requirements.txt\"\n",
    "\n",
    "-f https://download.pytorch.org/whl/torch_stable.html   \n",
    "celluloid==0.2.0   \n",
    "d2l==v0.16.0  \n",
    "IPython==7.16.1  \n",
    "numpy==1.19.5  \n",
    "matplotlib==3.3.4  \n",
    "torch==1.8.1+cu101  \n",
    "torchvision==0.9.1+cu101  \n",
    "scikit-learn==0.24.1  \n",
    "seaborn==0.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I inspect the [Docker file](https://github.com/aws/sagemaker-pytorch-training-toolkit/blob/master/docker/1.5.0/py3/Dockerfile.gpu) for the Sagemaker Pytorch container, I notice that it installs many of the libraries above such as:    \n",
    "iPython, Pytorch with cuda, torchvision, scikit-learn, numpy \n",
    "  \n",
    "  So for training in Sagemaker, I only need to then inject the missing libraries into the Sagemaker Pytorch container that my training code will depend on. The handy thing is that Sagemaker SDK can take a \"requirements.txt\" as input into the container startup and install specified libraries. All I need to do is to have the **\"requirements.txt\" file in the \"pytorch_script\" folder.**  The only additional library I need for trianing is d2l, which I'll include in my new \"requirements.txt\" file for training.\n",
    "    \n",
    "For this notebook, I will trim the list down to only those libraries that are required for data prep. So below is my new \"requirements.txt\" file contents:  \n",
    "  \n",
    "    \n",
    "Contents of my new \"requirements.txt\" for this notebook instance  \n",
    "IPython==7.16.1  \n",
    "numpy==1.19.5   \n",
    "scikit-learn==0.24.1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements-notebook.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I want to validate which of the original imports are still required into this notebook vs which ones are now redundant because of externalizing the training to Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Original imports\n",
    "\n",
    "### [Delete from original] from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import plotting libraries [Don't need since not doing training in Notebook]\n",
    "## import seaborn as sns \n",
    "## import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Pytorch [Don't need since not doing training in Notebook]\n",
    "## import torch\n",
    "## from torch import nn\n",
    "## from d2l import torch as d2l\n",
    "## import torchvision\n",
    "## from torch.utils import data\n",
    "## from torchvision import transforms\n",
    "## import torch.nn.functional as F\n",
    "\n",
    "# Import helper libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new imports\n",
    "import sagemaker\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Sagemaker to latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Training & Validation data for Sagemaker Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 3 cells don't change from the original. I still need to process the PKL training file, split it into Test and Validation sets. However afterwards I'll need to upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "bucketname = '[YOUR S3 BUCKET]' # replace with your S3 bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local data paths for temp storage of train and validation files\n",
    "train_dir = os.path.join(os.getcwd(), \"data/train\")\n",
    "test_dir = os.path.join(os.getcwd(), \"data/test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# S3 paths\n",
    "s3_prefix = \"pytorch-multiclass\"\n",
    "numpy_train_s3_prefix = f\"{s3_prefix}/data/train\"\n",
    "numpy_train_s3_uri = f\"s3://{bucketname}/{numpy_train_s3_prefix}\"\n",
    "numpy_test_s3_prefix = f\"{s3_prefix}/data/test\"\n",
    "numpy_test_s3_uri = f\"s3://{bucketname}/{numpy_test_s3_prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the original Training and Test data files in S3 (see my repo's main page for download links).\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucketname).download_file('pytorch-multiclass/data/train/DL1_Train.pkl', '../data/DL1_Train.pkl')\n",
    "\n",
    "final_train = pickle.load( open( \"../data/DL1_Train.pkl\", \"rb\" ), encoding='latin1')\n",
    "\n",
    "td = {'DRUM & BASS':0, 'R&B':1, 'BLUES':2, 'VOCAL JAZZ':3, 'NATURE SOUNDS':4, 'BAROQUE':5, 'DISNEY':6, 'HARD ROCK':7}\n",
    "\n",
    "X = np.array([final_train[key]['PACH'] for key in final_train.keys() if len(final_train[key]['text_genre']) == 1])\n",
    "y = np.array([td[final_train[key]['text_genre'][0]] for key in final_train.keys() if len(final_train[key]['text_genre']) == 1])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state = 8675309)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However now I need to do an additional step of serializing the Train and Val datasets and storing them to S3. I need to do that so that the Sagemaker training container, which runs outside of this notebook, can access the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Numpy\n",
    "np.save(os.path.join(train_dir, \"x_train.npy\"), X_train)\n",
    "np.save(os.path.join(test_dir, \"x_val.npy\"), X_val)\n",
    "np.save(os.path.join(train_dir, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(test_dir, \"y_val.npy\"), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the Training and Validation data to S3 ready for use by Sagemaker training\n",
    "\n",
    "s3_resource_bucket = boto3.Session().resource(\"s3\").Bucket(bucketname)\n",
    "\n",
    "s3_resource_bucket.Object(os.path.join(numpy_train_s3_prefix, \"x_train.npy\")).upload_file(\n",
    "    \"data/train/x_train.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_train_s3_prefix, \"y_train.npy\")).upload_file(\n",
    "    \"data/train/y_train.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_test_s3_prefix, \"x_val.npy\")).upload_file(\n",
    "    \"data/test/x_val.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_test_s3_prefix, \"y_val.npy\")).upload_file(\n",
    "    \"data/test/y_val.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Saparating training code and neural net definition from the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Sagemaker training container will run outside of this notebook, the model definition and training code will need to be accessible via separate files:\n",
    "- [pytorch_model_def.py](https://github.com/sjaffry/multiclass-classifier-pytorch-nn/blob/main/pytorch_script/pytorch_model_def.py)\n",
    "- [train_deploy_pytorch_without_dependencies.py](https://github.com/sjaffry/multiclass-classifier-pytorch-nn/blob/main/pytorch_script/train_deploy_pytorch_without_dependencies.py)  \n",
    "  \n",
    "Sagemaker training expect the above files along with any requirements.txt file in a single tar.gz archive. Having this tar.gz file in S3 means that I can now create a standarized CI/CD pipeline (MLOps), that will create a new tar.gz file each time I push a commit to any of the 3 files into my repo  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the above two files and the requirements.txt file and create a tar.gz and upload that to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/sjaffry/multiclass-classifier-pytorch-nn/main/pytorch_script/pytorch_model_def.py\n",
    "!wget -q https://raw.githubusercontent.com/sjaffry/multiclass-classifier-pytorch-nn/main/pytorch_script/train_deploy_pytorch_without_dependencies.py\n",
    "!wget -q https://raw.githubusercontent.com/sjaffry/multiclass-classifier-pytorch-nn/main/pytorch_script/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czf pytorch_script.tar.gz pytorch_model_def.py train_deploy_pytorch_without_dependencies.py requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource_bucket.Object(os.path.join(s3_prefix, \"pytorch_script.tar.gz\")).upload_file(\n",
    "    \"pytorch_script.tar.gz\"\n",
    ")\n",
    "\n",
    "code_file_uri = f\"s3://{bucketname}/{s3_prefix}/pytorch_script.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural net\n",
    "I will put the model definition in another separate file named: [**pytorch_model_def.py**](https://github.com/sjaffry/multiclass-classifier-pytorch-nn/blob/main/pytorch_script/pytorch_model_def.py) so I will not need the model definition in the notebook anymore  \n",
    "*(showing commented out code to make the comparison with previous - non-Sagemaker notebook easier)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = nn.Sequential(nn.Flatten(), \n",
    "#                     nn.Dropout(.2), \n",
    "#                     nn.Linear(4096, 1024), \n",
    "#                     nn.ReLU(),\n",
    "#                     nn.BatchNorm1d(1024), \n",
    "#                     nn.Dropout(.5), \n",
    "#                     nn.Linear(1024, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy function\n",
    "I will merge this function into the separate file I will create for training named: [**train_deploy_pytorch_without_dependencies.py**](https://github.com/sjaffry/multiclass-classifier-pytorch-nn/blob/main/pytorch_script/train_deploy_pytorch_without_dependencies.py)  \n",
    "*(showing commented out code to make the comparison with previous - non-Sagemaker notebook easier)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n",
    "#    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\n",
    "#    if isinstance(net, torch.nn.Module):\n",
    "#        net.eval()  # Set the model to evaluation mode\n",
    "#        if not device:\n",
    "#            device = next(iter(net.parameters())).device\n",
    "#    # No. of correct predictions, no. of predictions\n",
    "#    metric = d2l.Accumulator(2)\n",
    "#    for X, y in data_iter:\n",
    "#        if isinstance(X, list):\n",
    "#            # Required for BERT Fine-tuning (to be covered later)\n",
    "#            X = [x.to(device) for x in X]\n",
    "#        else:\n",
    "#            X = X.to(device)\n",
    "#        y = y.to(device)\n",
    "#        metric.add(d2l.accuracy(net(X), y), d2l.size(y))\n",
    "#    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "I will create a separate file named [**train_deploy_pytorch_without_dependencies.py**](https://github.com/sjaffry/multiclass-classifier-pytorch-nn/blob/main/pytorch_script/train_deploy_pytorch_without_dependencies.py), that will contain the training code below  \n",
    "*(showing commented out code to make the comparison with previous - non-Sagemaker notebook easier)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(net, train_iter, test_iter, num_epochs = 20, device=d2l.try_gpu(), lrate=0.005):\n",
    "#     \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\n",
    "#     def init_weights(m):\n",
    "#         if type(m) == nn.Linear:\n",
    "#             nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "#     net.apply(init_weights)\n",
    "#     print('training on', device)\n",
    "#     net.to(device)\n",
    "#     optimizer = torch.optim.Adam(net.parameters(), lr=lrate)\n",
    "#     loss = nn.CrossEntropyLoss()\n",
    "#     test_losses = []\n",
    "#     animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],\n",
    "#                             legend=['val acc'])\n",
    "#     timer = d2l.Timer()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         metric = d2l.Accumulator(2)\n",
    "#         net.train()\n",
    "#         for i, (X, y) in enumerate(final_train_loader):\n",
    "#             timer.start()\n",
    "#             optimizer.zero_grad()\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "#             y_hat = net(X)\n",
    "#             l = loss(y_hat, y)\n",
    "#             l.backward()\n",
    "#             optimizer.step()\n",
    "#             metric.add(l.sum(),  X.shape[0])\n",
    "#             timer.stop()\n",
    "#             train_loss = metric[0]/metric[1]\n",
    "#         test_acc = evaluate_accuracy_gpu(net, final_val_loader)\n",
    "#         animator.add(epoch+1, (test_acc))\n",
    "#     print('validation acc %.3f' % (test_acc))\n",
    "#     print('%.1f examples/sec on %s' % (metric[1]*num_epochs/timer.sum(), device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(8675309)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train_model(net, final_train_loader, final_val_loader, num_epochs = 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameters and start Sagemaker training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your AWS env parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# Useful SageMaker variables\n",
    "try:\n",
    "    # You're using a SageMaker notebook\n",
    "    sess = sagemaker.Session()\n",
    "    bucket = bucketname\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # You're using a notebook somewhere else\n",
    "    print(\"Setting role and SageMaker session manually...\")\n",
    "    bucket = bucketname\n",
    "    region = \"us-east-1\"\n",
    "\n",
    "    iam = boto3.client(\"iam\")\n",
    "    sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    sagemaker_execution_role_name = (\n",
    "        \"AmazonSageMaker-ExecutionRole-20191005T132574\"  # Change this to your role name\n",
    "    )\n",
    "    role = iam.get_role(RoleName=sagemaker_execution_role_name)[\"Role\"][\"Arn\"]\n",
    "    boto3.setup_default_session(region_name=region, profile_name=\"default\")\n",
    "    sess = sagemaker.Session(sagemaker_client=sagemaker_client, default_bucket=bucket)\n",
    "\n",
    "# Endpoint names\n",
    "pytorch_endpoint_name = \"pytorch-endpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start model training as a Sagemaker training job\n",
    "I'm using the Sagemaker PyTorch SDK to create a training job within Sagemaker Training. The PyTorch SDK will do the following:\n",
    "- create a new instance as per the parameter specification\n",
    "- download your training and validation data from S3 (numpy_train_s3_uri, numpy_test_s3_uri)\n",
    "- download your custom training code and model definition from S3 (pytorch_script.tar.gz)\n",
    "- download and start a PyTorch container\n",
    "- Inject your custom training code, mode definition and training and validation data to run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"epochs\": 50, \"batch_size\": 100, \"learning_rate\": 0.01}\n",
    "\n",
    "train_instance_type = \"ml.g4dn.xlarge\"\n",
    "inputs = {\"train\": numpy_train_s3_uri, \"test\": numpy_test_s3_uri}\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"train_deploy_pytorch_without_dependencies.py\",\n",
    "    \"source_dir\": code_file_uri,\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"pytorch-model\",\n",
    "    \"framework_version\": \"1.10\",\n",
    "    \"py_version\": \"py38\",\n",
    "}\n",
    "\n",
    "estimator = PyTorch(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
